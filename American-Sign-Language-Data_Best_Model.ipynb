{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL Competition Winning Model : American_sign_language_image_classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading https://files.pythonhosted.org/packages/93/11/f7bc11a9527c52a31695816e7f38d421d8da0d096beff3fa2638dfbd4d95/kaggle-1.4.2.tar.gz (43kB)\n",
      "\u001b[K    100% |################################| 51kB 2.3MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.23.0,>=1.15 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from kaggle)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from kaggle)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/fastai/lib/python3.6/site-packages (from kaggle)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/envs/fastai/lib/python3.6/site-packages (from kaggle)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/fastai/lib/python3.6/site-packages (from kaggle)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/fastai/lib/python3.6/site-packages (from kaggle)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from requests->kaggle)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from requests->kaggle)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Running setup.py bdist_wheel for kaggle ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3b/8e/14/adcbb71d126f5fb939fec8aa81394450922c20eee876bf8882\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.4.2\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/68/12/4cabc5c01451eb3b413d19ea151f36e33026fc0efb932bf51bcaf54acbf5/Keras-2.2.0-py2.py3-none-any.whl (300kB)\n",
      "\u001b[K    100% |################################| 307kB 1.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/fastai/lib/python3.6/site-packages (from keras)\n",
      "Collecting keras-applications==1.0.2 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl (43kB)\n",
      "\u001b[K    100% |################################| 51kB 4.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing==1.0.1 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
      "Collecting h5py (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/cb/726134109e7bd71d98d1fcc717ffe051767aac42ede0e7326fd1787e5d64/h5py-2.8.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
      "\u001b[K    100% |################################| 2.8MB 238kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: h5py, keras-applications, keras-preprocessing, keras\n",
      "Successfully installed h5py-2.8.0 keras-2.2.0 keras-applications-1.0.2 keras-preprocessing-1.0.1\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: requests in /opt/conda/envs/fastai/lib/python3.6/site-packages\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from requests)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from requests)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from requests)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from requests)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "!pip install keras\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "time taken: 0.0028104782104492188\n",
      "/notebookssign-language-mnist.zip file is downloaded !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Function to get the data from the kaggle link\n",
    "'''\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "def getdata(file_url, target_path_final):\n",
    "    #file_url=\"https://www.kaggle.com/datamunge/sign-language-mnist/downloads/sign-language-mnist.zip/1\"\n",
    "    r=requests.get(file_url, stream=True)\n",
    "    print(r)\n",
    "    #target_path=os.getcwd()\n",
    "    start_time=time.time()\n",
    "    #target_path_final=target_path+\"\\\\datasets\\\\\"+\"set00.tar\"\n",
    "    with open(target_path_final, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1048576):\n",
    "            if chunk:f.write(chunk)\n",
    "    end_time=time.time()\n",
    "    print(\"time taken: {}\".format(end_time-start_time))\n",
    "    print(\"%s file is downloaded !\\n\"%target_path_final)\n",
    "target_path=os.getcwd()\n",
    "getdata(\"https://www.kaggle.com/datamunge/sign-language-mnist/downloads/sign-language-mnist.zip/1\",target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data is preesent through Local kaggle Api and accessed through the terminal iteselt [easiest way]\n",
    "'''\n",
    "import csv\n",
    "path=os.getcwd()\n",
    "#path=\"C:\\\\cygwin64\\\\home\\\\sananand\\\\.kaggle\\\\datasets\\\\datamunge\\\\sign-language-mnist\\\\\"\n",
    "file_train=path+\"/sign_mnist_train.csv\"\n",
    "file_test=path+\"/sign_mnist_test.csv\"\n",
    "train_data, test_data=[],[]\n",
    "y_train,y_test=[],[]\n",
    "with open(file_train, 'rt', encoding=\"utf8\") as file_train:\n",
    "    reader_train=csv.reader(file_train, delimiter=' ', quotechar='|')\n",
    "    for idx, row in enumerate(reader_train):\n",
    "        if idx!=0:\n",
    "            temp=row[0].split(',')\n",
    "            tempnew=[int(i) for idx,i in enumerate(temp) if idx!=0]\n",
    "            y_train.append(int(temp[0]))\n",
    "            train_data.append(tempnew)\n",
    "with open(file_test, 'rt', encoding=\"utf8\") as file_test:\n",
    "    reader_test=csv.reader(file_test, delimiter=' ', quotechar='|')\n",
    "    for idx, row in enumerate(reader_test):\n",
    "        if idx!=0:\n",
    "            temp=row[0].split(',')\n",
    "            tempnew=[int(i) for idx,i in enumerate(temp) if idx!=0]\n",
    "            y_test.append(int(temp[0]))\n",
    "            test_data.append(tempnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Traning Examples 27455\n",
      "Number of Test Examples 7172\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Number of Traning Examples\", len(train_data))\n",
    "print(\"Number of Test Examples\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array=np.asarray(train_data)\n",
    "test_array=np.asarray(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 784)\n"
     ]
    }
   ],
   "source": [
    "print(train_array.shape)\n",
    "number_of_samples = train_array.shape[0]\n",
    "height=28\n",
    "width=28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array_transformed=np.reshape(train_array, (number_of_samples, height, width, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27455, 28, 28, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_array_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7172, 784)\n"
     ]
    }
   ],
   "source": [
    "print(test_array.shape)\n",
    "test_array_transformed=np.reshape(test_array, (test_array.shape[0], height, width, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7172, 28, 28, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/86/95/274190b39950e1e9eef4b071acefea832ac3e2c19bb4b442fa54f3214d2e/tensorflow-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (51.1MB)\n",
      "\u001b[K    100% |################################| 51.1MB 13kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/13/b05bbd05d9f0c19e5746f149a285aa81f7c353e231ffecdb237c6e2ad3cc/grpcio-1.13.0-cp36-cp36m-manylinux1_x86_64.whl (9.1MB)\n",
      "\u001b[K    100% |################################| 9.1MB 76kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.10.0,>=1.9.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/1f/3da43860db614e294a034e42d4be5c8f7f0d2c75dc1c428c541116d8cdab/tensorboard-1.9.0-py3-none-any.whl (3.3MB)\n",
      "\u001b[K    100% |################################| 3.3MB 218kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/96/5d/18feb90462c8edaae71305716c7e8bac479fc9dface63221f808a6b95880/absl-py-0.3.0.tar.gz (84kB)\n",
      "\u001b[K    100% |################################| 92kB 6.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools<=39.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
      "\u001b[K    100% |################################| 573kB 1.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting protobuf>=3.4.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/f0/db040681187496d10ac50ad167a8fd5f953d115b16a7085e19193a6abfd2/protobuf-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
      "\u001b[K    100% |################################| 7.1MB 97kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.10 (from tensorboard<1.10.0,>=1.9.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n",
      "\u001b[K    100% |################################| 327kB 2.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.10.0,>=1.9.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |################################| 81kB 6.4MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: absl-py, gast\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/4c/16/ef/e36a23f2432e9220f8845f94e2c3abd39e7d9d1cd458d3159d\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
      "Successfully built absl-py gast\n",
      "Installing collected packages: grpcio, werkzeug, markdown, setuptools, protobuf, tensorboard, astor, absl-py, gast, tensorflow\n",
      "  Found existing installation: setuptools 39.2.0\n",
      "    Uninstalling setuptools-39.2.0:\n",
      "      Successfully uninstalled setuptools-39.2.0\n",
      "Successfully installed absl-py-0.3.0 astor-0.7.1 gast-0.2.0 grpcio-1.13.0 markdown-2.6.11 protobuf-3.6.0 setuptools-39.1.0 tensorboard-1.9.0 tensorflow-1.9.0 werkzeug-0.14.1\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# define image data generator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "X_train = train_array_transformed / 255.0\n",
    "X_test = test_array_transformed / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import np_utils\n",
    "# one hot encode\n",
    "y_train=np_utils.to_categorical(y_train)\n",
    "y_test=np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 25)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "number_of_classes=y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "network=models.Sequential()\n",
    "network.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(28,28,1,)))\n",
    "network.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "network.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "network.add(layers.Dropout(0.3))\n",
    "network.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "network.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "network.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "network.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "network.add(layers.Dropout(0.3))\n",
    "network.add(layers.Flatten())\n",
    "network.add(layers.Dense(1500, activation='relu'))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(number_of_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1500)              1729500   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                37525     \n",
      "=================================================================\n",
      "Total params: 2,007,281\n",
      "Trainable params: 2,007,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  if sys.path[0] == '':\n",
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[], verbose=2, steps_per_epoch=857, epochs=10)`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 92s - loss: 1.8232 - acc: 0.4233 - val_loss: 0.3093 - val_acc: 0.8926\n",
      "Epoch 2/10\n",
      " - 89s - loss: 0.3935 - acc: 0.8665 - val_loss: 0.0941 - val_acc: 0.9713\n",
      "Epoch 3/10\n",
      " - 90s - loss: 0.2083 - acc: 0.9310 - val_loss: 0.0292 - val_acc: 0.9926\n",
      "Epoch 4/10\n",
      " - 90s - loss: 0.1473 - acc: 0.9508 - val_loss: 0.0126 - val_acc: 0.9971\n",
      "Epoch 5/10\n",
      " - 91s - loss: 0.1153 - acc: 0.9620 - val_loss: 0.0269 - val_acc: 0.9943\n",
      "Epoch 6/10\n",
      " - 90s - loss: 0.0931 - acc: 0.9691 - val_loss: 0.0040 - val_acc: 0.9994\n",
      "Epoch 7/10\n",
      " - 91s - loss: 0.0923 - acc: 0.9709 - val_loss: 0.0107 - val_acc: 0.9944\n",
      "Epoch 8/10\n",
      " - 93s - loss: 0.0808 - acc: 0.9730 - val_loss: 0.0113 - val_acc: 0.9941\n",
      "Epoch 9/10\n",
      " - 91s - loss: 0.0727 - acc: 0.9767 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 92s - loss: 0.0678 - acc: 0.9797 - val_loss: 0.0135 - val_acc: 0.9944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ad03a7198>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model compilation\n",
    "import keras.optimizers\n",
    "network.compile(optimizer='adam',\n",
    "               loss = 'categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "# keras.optimizers.RMSprop(lr=5e-4)\n",
    "# Fit the model\n",
    "network.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        samples_per_epoch=X_train.shape[0],\n",
    "                        nb_epoch=10, validation_data=(X_test, y_test),\n",
    "                        callbacks=[],verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
